# LangNet Tasks Configuration
# Based on CrewAI task definition format

# Document Analysis Tasks
analyze_document:
  description: >
    [Document Analysis] Extract ALL information from provided DOCUMENTS and INSTRUCTIONS.

    YOU RECEIVE 2 INPUT SOURCES:

    SOURCE 1 - DOCUMENTS (PRIMARY):
    - document_content: {document_content}
      This contains FULL TEXT extracted from uploaded files (PDFs, DOCX, etc.)
      May be divided into CHUNKS if long (separated by "---CHUNK---")
      This is the PRIMARY source of FACTUAL information about current state

    SOURCE 2 - INSTRUCTIONS (CONTEXT):
    - additional_instructions: {additional_instructions}
      This contains objectives, goals, context provided by the user
      This provides INTENT and PURPOSE for the system being built
      This is SECONDARY but important for understanding desired state

    CRITICAL:
    - document_content is ALREADY EXTRACTED - work directly with the text provided
    - DO NOT try to read files or use document_reader tool
    - Process ALL chunks if present (they are parts of same logical document)

    YOUR TASK: Analyze BOTH sources to understand the complete picture.

    STEP 1 - READ BOTH SOURCES:

    (A) Read ENTIRE document_content:
        - Read all text including all chunks if divided
        - This tells you WHAT EXISTS TODAY and WHAT PROBLEMS exist
        - Extract FACTS, NUMBERS, NAMES, CURRENT PROCESSES from actual text

    (B) Read additional_instructions:
        - This tells you WHAT THEY WANT TO BUILD and WHY
        - Extract GOALS, OBJECTIVES, DESIRED FEATURES from instructions
        - Understand the VISION for the new system

    STEP 2 - EXTRACT FROM DOCUMENTS (document_content):

    From the actual text, extract:

    (1) STAKEHOLDERS & ACTORS:
        - Names, roles, companies mentioned in text
        - Teams, departments, user types described
        - Current and future actors

    (2) BUSINESS CONTEXT:
        - What business/organization is this for?
        - What industry/domain/sector?
        - What geography/region if mentioned?
        - Current situation, background

    (3) CURRENT PAIN POINTS:
        - Explicit problems mentioned in documents
        - Inefficiencies, bottlenecks, frustrations
        - Manual/repetitive/time-consuming work
        - What doesn't work well today?

    (4) CURRENT PROCESS & TOOLS:
        - How do they work today?
        - What tools/systems currently used?
        - What is the current workflow?
        - Team size, structure mentioned
        - Volumes, frequencies, metrics

    (5) QUANTITATIVE DATA (CRITICAL):
        - ALL NUMBERS: volumes, sizes, frequencies, counts
        - Performance metrics, success rates, percentages
        - Timings, durations, costs
        - Team sizes, resource counts

    (6) DOMAIN TERMINOLOGY:
        - Technical terms specific to their domain
        - Business rules, regulations mentioned
        - Data entities described
        - Workflows/processes detailed

    STEP 3 - EXTRACT FROM INSTRUCTIONS (additional_instructions):

    From the instructions, extract:

    (1) PROJECT GOALS:
        - What should the system achieve?
        - What problems should it solve?
        - Expected outcomes

    (2) DESIRED FEATURES:
        - What functionalities are requested?
        - What should the system do?
        - Modules or components mentioned

    (3) SYSTEM VISION:
        - What type of system (web app, mobile, API, desktop, etc.)?
        - Architecture hints or preferences
        - Technology preferences if mentioned

    (4) CONSTRAINTS:
        - Timeline, budget mentioned
        - Technical limitations
        - Regulatory requirements

    STEP 4 - COMBINE UNDERSTANDING:

    Merge insights from BOTH sources:
    - Documents tell you CURRENT STATE (as-is)
    - Instructions tell you DESIRED STATE (to-be)
    - Together they define what needs to be built

    STEP 5 - IDENTIFY DOMAIN:

    From both sources, determine:
    - Primary industry/sector
    - Type of application needed
    - Geographic context (if relevant for compliance)
    - Key technologies mentioned or implied

    IMPORTANT:
    - Extract ONLY what is in the text - do NOT invent
    - Use VERBATIM QUOTES as evidence
    - If information not present, state "not mentioned"
    - Process ALL chunks if document is divided
  expected_output: >
    JSON object with analysis from BOTH documents and instructions.

    Structure: Top-level object containing the following fields:
    - domain_identified: string describing primary industry or sector
    - from_documents: object with nested fields
      * stakeholders: array of strings with quotes from documents
      * business_context: string with key facts
      * pain_points: array of strings with evidence
      * current_process: string describing how they work
      * current_tools: array of tools mentioned
      * quantitative_data: array of numbers with verbatim quotes
      * domain_terminology: array of technical terms
    - from_instructions: object with nested fields
      * project_goals: array of goals
      * desired_features: array of features
      * system_vision: string describing system type
      * constraints: array of limitations
    - synthesis: object with nested fields
      * current_state: string with as-is summary
      * desired_state: string with to-be summary
      * gap: string describing what needs to change
    - extraction_status: string value "success" or "failed"
    - words_processed: integer count

extract_requirements:
  description: >
    [Requirements Extraction] Extract requirements from DOCUMENTS + INSTRUCTIONS, then INFER technical needs.

    YOU RECEIVE 3 INPUT SOURCES:
    - document_content: {document_content} (factual information from uploaded files)
    - additional_instructions: {additional_instructions} (project goals and context)
    - analysis_json: {analysis_json} (structured analysis from previous step)
    - project_name: {project_name}

    YOUR TASK HAS 4 PARTS:
    PART 1: Extract from DOCUMENTS
    PART 2: Extract from INSTRUCTIONS
    PART 3: INFER technical requirements
    PART 4: Prepare for WEB RESEARCH

    ═══════════════════════════════════════════════════════════
    PART 1: EXTRACT FROM DOCUMENTS (document_content)
    ═══════════════════════════════════════════════════════════

    From ACTUAL TEXT in documents, extract requirements:

    FUNCTIONAL REQUIREMENTS from documents:
    - MANUAL TASK mentioned → FR to automate it
    - PAIN POINT mentioned → FR to solve it
    - DATA/ENTITY mentioned → CRUD FRs
    - INTEGRATION mentioned → Integration FR
    - WORKFLOW described → FRs for each step

    For EACH FR from documents:
    - Provide VERBATIM QUOTE as evidence
    - Mark source: "from_document"

    NON-FUNCTIONAL REQUIREMENTS from documents:
    - VOLUME/SCALE mentioned → Performance NFR with that number
    - SPEED issues mentioned → Response time NFR
    - TEAM SIZE mentioned → Usability NFR
    - SENSITIVE DATA mentioned → Security NFR

    ═══════════════════════════════════════════════════════════
    PART 2: EXTRACT FROM INSTRUCTIONS (additional_instructions)
    ═══════════════════════════════════════════════════════════

    From instructions provided by user:

    FUNCTIONAL REQUIREMENTS from instructions:
    - FEATURE requested → FR
    - MODULE described → FRs for that module
    - WORKFLOW described → FRs for workflow steps

    For EACH FR from instructions:
    - Quote the instruction text
    - Mark source: "from_instructions"

    ═══════════════════════════════════════════════════════════
    CRITICAL - REQUIREMENT EXTRACTION LOGIC
    ═══════════════════════════════════════════════════════════

    YOUR PRIMARY SOURCE for functional requirements is INSTRUCTIONS (additional_instructions).

    STEP 1: Read additional_instructions
    - Identify each MODULE, FEATURE, or FUNCTIONALITY explicitly requested
    - Each one becomes a separate FR
    - Description should match what was requested (not generic "automation")
    - Example: If instructions say "Cadastro Inteligente do Portfólio", FR should be "Cadastro Inteligente do Portfólio", NOT "Automate portfolio management"

    STEP 2: Read document_content to ENRICH requirements
    - Look for PAIN POINTS that relate to the instructions
    - Look for WORKFLOWS that should be automated
    - Look for SPECIFIC CONSTRAINTS or REQUIREMENTS mentioned
    - Use these to add evidence and context, NOT to replace instruction-based FRs

    STEP 3: Combine both sources
    - FR description = What was requested in instructions (preserve original wording)
    - FR evidence = Quote from documents showing WHY it's needed or HOW it's currently done
    - FR context/details = Specific data from documents (volumes, names, locations)

    EXAMPLE OF CORRECT EXTRACTION:

    additional_instructions says: "Agente de IA para Captura e Leitura dos Certames"
    document_content says: "Farmac needs to monitor public procurement notices across Bahia, Sergipe, and Alagoas. Current manual process with 2-3 people."

    ✅ CORRECT:
    Requirement object with fields:
    - id: "FR-002"
    - description: "Agente de IA para captura e leitura dos certames de fontes públicas (federal, estaduais e municipais)"
    - source: "from_instructions"
    - evidence: "Current manual process with 2-3 person team monitoring procurement notices. Geographic scope: Bahia, Sergipe, and Alagoas."
    - priority: "high"
    - context: nested object with current_team_size "2-3 people", geographic_scope "Bahia, Sergipe, Alagoas", company "Farmac"

    ❌ WRONG (too generic, ignores instructions):
    Requirement with:
    - id: "FR-002"
    - description: "Automate the manual task of capturing procurement notices"
    - source: "from_document"
    - evidence: "Manual monitoring needed"

    ═══════════════════════════════════════════════════════════
    HANDLING SPECIFIC DATA FROM DOCUMENTS
    ═══════════════════════════════════════════════════════════

    IF documents mention specific data, use it APPROPRIATELY:

    - Company name (e.g., "Farmac") → Include in:
      * project_context section (NOT in every FR description)
      * actors/stakeholders
      * evidence field when relevant

    - Specific volumes (e.g., "10,000 items") → Include in:
      * NFR for performance/scalability
      * Context field of related FRs
      * Evidence when showing scale of problem

    - Locations (e.g., "Bahia, Sergipe, Alagoas") → Include in:
      * Scope definition
      * Geographic filtering requirement
      * Context of relevant FRs

    - People names (e.g., "Douglas") → Include in:
      * Actors/stakeholders section
      * NOT in requirement descriptions

    DO NOT force specific data into every requirement.
    USE specific data to make requirements realistic and contextual.

    ═══════════════════════════════════════════════════════════
    PART 3: INFER TECHNICAL REQUIREMENTS (not explicitly stated)
    ═══════════════════════════════════════════════════════════

    Based on extracted requirements, INFER necessary technical requirements:

    INFER DATA REQUIREMENTS:
    - Entities mentioned → Database schema needed
    - Large volumes → Indexing, optimization needed

    INFER INFRASTRUCTURE:
    - Web application → Hosting needed
    - API mentioned → API architecture needed

    INFER SECURITY:
    - User data → Authentication needed
    - Sensitive data → Encryption needed

    INFER MONITORING:
    - Production system → Logging needed
    - Critical operations → Error handling needed

    For EACH inferred requirement:
    - Mark source: "inferred"
    - Provide RATIONALE

    ═══════════════════════════════════════════════════════════
    PART 3.5: SUGGEST CRITICAL MISSING REQUIREMENTS (MANDATORY)
    ═══════════════════════════════════════════════════════════

    YOU MUST SUGGEST 5-10 CRITICAL REQUIREMENTS NOT MENTIONED IN SOURCES.

    For a production-ready system in this domain, CREATE requirements for:

    1. LEGAL COMPLIANCE (2-3 requirements):
       - IF Brazil detected → LGPD compliance (data privacy, consent, deletion rights)
       - IF EU detected → GDPR compliance
       - IF healthcare → HIPAA compliance
       - Audit trail for compliance reporting

    2. OPERATIONAL EXCELLENCE (2-3 requirements):
       - Automated backup and disaster recovery (RTO/RPO defined)
       - System monitoring and alerting (uptime, errors, performance)
       - Logging and audit trail for all critical operations

    3. SECURITY (1-2 requirements):
       - Two-factor authentication (2FA) for administrative access
       - Rate limiting to prevent abuse/DoS
       - Data encryption at rest and in transit

    4. PERFORMANCE & SCALABILITY (1-2 requirements):
       - Caching strategy for frequently accessed data
       - Database query optimization and indexing
       - Load balancing for high availability

    5. USER EXPERIENCE (1 requirement):
       - Mobile responsiveness or accessibility (WCAG)

    FORMAT FOR EACH SUGGESTED REQUIREMENT:
    {
      "id": "NFR-XXX" or "FR-XXX",
      "description": "Short requirement title",
      "source": "suggested_by_ai",
      "rationale": "Explain WHY this is critical for this specific domain. Reference standards, regulations, or common problems in similar systems.",
      "priority": "high" or "medium"
    }

    EXAMPLE - Brazil context, public procurement domain:
    {
      "id": "NFR-010",
      "description": "Conformidade com LGPD para dados de licitações e fornecedores",
      "source": "suggested_by_ai",
      "rationale": "Sistema processa dados sensíveis de empresas e órgãos públicos incluindo CNPJs, contatos, e histórico de licitações. LGPD (Lei 13.709/2018) exige consentimento explícito, direito ao esquecimento, e relatório de impacto à privacidade. Não-conformidade pode resultar em multas de até 2% do faturamento. Crítico para operação legal no Brasil.",
      "priority": "high"
    }

    CRITICAL: You MUST generate at least 5 suggested requirements. If you generate less than 5, the validation will FAIL.

    ═══════════════════════════════════════════════════════════
    PART 4: PREPARE FOR WEB RESEARCH + CONTEXT EXTRACTION
    ═══════════════════════════════════════════════════════════

    STEP 4A: EXTRACT BUSINESS CONTEXT FOR DOCUMENT (STRUCTURED JSON)

    From documents, extract and CREATE business_context object in your JSON output:

    business_context: {
      "geographic_scope": [array of strings],
      "industry": "string",
      "company_type": "string",
      "products_services": [array of strings],
      "target_market": "string",
      "regulatory_bodies": [array of strings],
      "domain_terminology": [
        {"term": "string", "definition": "string"},
        ...
      ],
      "quantitative_data": {
        "team_size": "string or number",
        "portfolio_size": "string or number",
        "market_coverage": "string",
        "other_metrics": {}
      }
    }

    EXTRACTION RULES:

    1. GEOGRAPHIC SCOPE - Extract ALL locations mentioned:
       Example: ["Bahia", "Sergipe", "Alagoas", "Brazil"]
       If no locations: ["Not specified"]

    2. INDUSTRY - Single string with primary sector:
       Example: "Healthcare - Clinical Laboratory Supplies"
       Example: "Public Procurement - Government Bidding"

    3. COMPANY TYPE - What type of company:
       Example: "Distributor", "Manufacturer", "Service Provider", "Platform", "Marketplace"

    4. PRODUCTS/SERVICES - Array of main offerings:
       Example: ["Laboratory reagents", "Clinical analysis equipment", "Hospital supplies"]

    5. TARGET MARKET - Who are the customers:
       Example: "B2G (Business-to-Government) - Public hospitals and laboratories"

    6. REGULATORY BODIES - Extract ALL mentioned:
       Example: ["ANVISA", "Ministry of Health"]
       If none: []

    7. DOMAIN TERMINOLOGY - Extract 3-5 KEY terms with definitions:
       Example: [
         {"term": "Comodato", "definition": "Equipment loan contract where supplier provides equipment and consumables at unit price without fixed rental"},
         {"term": "Licitação", "definition": "Public procurement process for goods and services"},
         {"term": "Edital", "definition": "Public tender notice document with requirements"}
       ]

    8. QUANTITATIVE DATA - Extract specific numbers:
       Example: {
         "team_size": "2-3 people",
         "portfolio_size": "10,000 ANVISA-registered items",
         "market_coverage": "3 Brazilian states"
       }

    This structured context will populate the "Context and Justification" section of the requirements document.

    STEP 4B: FORMULATE WEB RESEARCH QUERIES

    Identify domain and formulate 8-15 search queries.

    Make queries SPECIFIC to identified domain AND geography.

    LANGUAGE STRATEGY:
    - IF geographic context includes Brazil/Brasil/Portuguese → Use PORTUGUESE queries
    - IF geographic context includes Spanish-speaking countries → Use SPANISH queries
    - OTHERWISE → Use ENGLISH queries

    EXAMPLE - Brazilian context detected:
    - "melhores práticas licitações públicas brasil"
    - "Lei 14.133 requisitos sistema licitação"
    - "integração ComprasNet API brasil"
    - "sistemas gestão licitações saúde brasil"

    EXAMPLE - US context detected:
    - "government procurement software best practices USA"
    - "FAR compliance requirements procurement systems"

    QUERY CATEGORIES (adapt to domain):
    1. Best practices in [domain] + [country/region]
    2. Legal/regulatory requirements [domain] + [country]
    3. Industry standards and compliance [domain]
    4. Similar systems/software [domain] + [country]
    5. Technical architecture patterns [domain]
    6. Integration standards [domain-specific systems]
    7. Security requirements [domain] + [country regulations]
    8. Performance benchmarks [domain]

    ═══════════════════════════════════════════════════════════
    FINAL VALIDATION - CHECKLIST BEFORE RETURNING OUTPUT
    ═══════════════════════════════════════════════════════════

    Before generating your output, COUNT and verify:

    ✓ Each MODULE/FEATURE from additional_instructions has a corresponding FR
    ✓ FR descriptions match the REQUESTED features (not generic "automation")
    ✓ Evidence cites document_content showing WHY each requirement is needed
    ✓ Specific data (company, volumes, locations) is in APPROPRIATE sections
    ✓ I inferred technical requirements (database, API, security, monitoring)
    ✓ Each inferred requirement has RATIONALE explaining why necessary

    ✓ I suggested AT LEAST 5 requirements with source "suggested_by_ai"
      COUNT THEM: ___ suggested requirements (must be >= 5)
      - At least 2 for legal compliance (LGPD/GDPR/HIPAA/audit)
      - At least 2 for operational excellence (backup/monitoring/logging)
      - At least 1 for security (2FA/rate limiting/encryption)

    ✓ I extracted business_context object with ALL fields populated:
      - geographic_scope: array with locations
      - industry: string
      - company_type: string
      - regulatory_bodies: array (e.g., ANVISA, FDA)
      - domain_terminology: array of term/definition objects

    ✓ I prepared 8-15 web_research_queries in APPROPRIATE LANGUAGE
      - Portuguese if Brazil/Brasil detected
      - Spanish if Hispanic countries detected
      - English otherwise

    ✓ Web queries are SPECIFIC to domain + geography (not generic)

    RED FLAGS - DO NOT do this:
    ❌ FR says "Automate procurement" when instruction said "Agente de IA para captura"
    ❌ Company name appears in every FR description unnecessarily
    ❌ Generic "user login" FR when not requested in instructions
    ❌ Missing FRs for features explicitly requested in instructions
    ❌ Requirements with no source/evidence citation
    ❌ Invented stakeholders/companies not mentioned in documents
    ❌ Placeholder text like "TBD", "to be defined", "N/A" without explanation

    QUALITY CHECK - Count your FRs:
    - If additional_instructions lists 4 modules → you should have ~4+ FRs from instructions
    - If you have many generic FRs but few instruction-based ones → REVIEW AGAIN

    If ANY checkbox is unchecked, REVIEW inputs again before returning.

  expected_output: >
    JSON with requirements from 4 sources (documents, instructions, inferred, suggested).

    Structure: Top-level object with the following fields:

    - functional_requirements: array of requirement objects, each containing:
      * id: string like "FR-001", "FR-002", etc
      * description: string with requirement description
      * source: string value "from_document" or "from_instructions" or "inferred" or "suggested_by_ai"
      * evidence: string with verbatim quote (if from doc/instructions)
      * rationale: string with explanation (if inferred or suggested)
      * priority: string value "high" or "medium" or "low"

    - non_functional_requirements: array with same structure as functional_requirements

    - business_rules: array of business rule objects

    - entities: array of data entity objects

    - actors: array of actor/stakeholder objects with name and role

    - workflows: array of workflow objects

    - business_context: object containing:
      * geographic_scope: array of locations (countries, states, cities)
      * industry: string describing industry/sector
      * company_type: string (e.g., distributor, manufacturer)
      * products_services: array of products/services offered
      * target_market: string describing target customers
      * regulatory_bodies: array of regulatory bodies mentioned (e.g., ANVISA, FDA)
      * domain_terminology: array of objects with term and definition
      * quantitative_data: object with key business metrics

    - web_research_queries: array of strings with search queries in APPROPRIATE LANGUAGE for next step

research_additional_info:
  description: >
    [Web Research] Find ANALOGOUS SYSTEMS and BEST PRACTICES to enrich requirements.

    YOU RECEIVE:
    - requirements_json: {requirements_json} (extracted + inferred requirements)
    - web_research_queries: Suggested queries from previous step

    GOAL: Research similar/analogous systems to find:
    1. Features we might have missed
    2. Industry standards and best practices
    3. Technical recommendations
    4. Compliance requirements

    ═══════════════════════════════════════════════════════════
    STEP 1: UNDERSTAND THE SYSTEM TYPE
    ═══════════════════════════════════════════════════════════

    From requirements_json understand:
    - What domain/industry?
    - What type of system?
    - Core functionalities?
    - Key challenges?

    ═══════════════════════════════════════════════════════════
    STEP 2: SEARCH FOR ANALOGOUS/SIMILAR SYSTEMS
    ═══════════════════════════════════════════════════════════

    Use serper_search tool to find similar systems:

    (A) EXISTING SOLUTIONS:
        Search: "[domain] [system type] software"
        Search: "open source [analogous system]"
        Goal: Find what features similar systems have

    (B) INDUSTRY STANDARDS:
        Search: "[domain] software best practices"
        Search: "[domain] system requirements"
        Goal: Identify standard requirements

    (C) TECHNICAL ARCHITECTURE:
        Search: "[system type] architecture patterns"
        Search: "technology stack for [use case]"
        Goal: Find recommended tech and patterns

    (D) COMPLIANCE:
        Search: "[domain] compliance requirements"
        Search: "[domain] regulations [country if identified]"
        Goal: Identify regulatory requirements

    (E) PERFORMANCE:
        Search: "[system type] performance benchmarks"
        Search: "[domain] SLA standards"
        Goal: Find realistic performance targets

    IMPORTANT:
    - Use serper_search for EACH query
    - Adapt queries to domain context
    - If domain has geographic specificity, add country to queries

    ═══════════════════════════════════════════════════════════
    STEP 3: EXTRACT INSIGHTS
    ═══════════════════════════════════════════════════════════

    From search results extract:

    (1) FEATURES from analogous systems
    (2) BEST PRACTICES for this domain
    (3) TECHNICAL RECOMMENDATIONS
    (4) COMPLIANCE REQUIREMENTS
    (5) PERFORMANCE BASELINES

    ═══════════════════════════════════════════════════════════
    STEP 4: IDENTIFY GAPS
    ═══════════════════════════════════════════════════════════

    Compare findings with requirements_json:
    - What features are common in similar systems but missing?
    - What compliance requirements apply but weren't identified?
    - What technical requirements are standard but not included?

    ═══════════════════════════════════════════════════════════
    ADAPT TO CONTEXT
    ═══════════════════════════════════════════════════════════

    If requirements indicate specific geography/regulations:
    - Add country/region to search queries
    - Search for local regulations
    - Find region-specific standards

    Example: If Brazil context evident, add "brasil" to queries

  expected_output: >
    JSON with web research findings.

    Structure: Top-level object with the following fields:

    - analogous_systems: array of system objects, each containing:
      * name: string with system name
      * description: string describing what it does
      * source_url: string with URL
      * key_features: array of feature strings
      * relevance: string explaining why similar

    - best_practices: array of best practice objects with sources

    - recommended_technologies: array of technology recommendation objects

    - compliance_requirements: array of compliance requirement objects

    - performance_benchmarks: object with benchmark data

    - potentially_missing_requirements: array of requirement objects, each containing:
      * type: string value "FR" or "NFR" or "BR"
      * description: string with requirement description
      * justification: string like "Found in X similar systems"
      * source: string with URL

validate_requirements:
  description: >
    [Requirements Validation and Final Document Generation] Validate extracted requirements and generate professional requirements document.

    YOU WILL RECEIVE:
    - requirements_json: {requirements_json} (all extracted requirements)
    - research_findings_json: {research_findings_json} (web research results)
    - template: {template} (Markdown template for final document)
    - Project: {project_name}

    CRITICAL INSTRUCTIONS FOR DOCUMENT GENERATION:
    You are generating the FINAL REQUIREMENTS DOCUMENT that will be presented to stakeholders.
    This document MUST be:
    - COMPLETE (all sections filled with real data)
    - PROFESSIONAL (ready for stakeholder review)
    - ACCURATE (based on actual extracted requirements)
    - TRACEABLE (every requirement linked to source)

    DO NOT USE PLACEHOLDER TEXT:
    - NO "To be filled by analysis"
    - NO "TBD" or "N/A" without explanation
    - NO "Lorem ipsum" or generic examples
    - If data is missing for a section, explicitly state what is missing and why

    ═══════════════════════════════════════════════════════════
    STEP 0 - VALIDATE COMPLETENESS FROM 4 SOURCES
    ═══════════════════════════════════════════════════════════

    Requirements should come from 4 SOURCES:

    SOURCE 1 - DOCUMENTS (from document_content):
    ✅ Every major concept from documents has requirements
    ✅ Quantitative data from documents is reflected
    ✅ Tools/systems mentioned have integration requirements
    ✅ Pain points mentioned have solution requirements
    ✅ Each has source citation with verbatim quote

    SOURCE 2 - INSTRUCTIONS (from additional_instructions):
    ✅ All requested features have FRs
    ✅ All modules described have FRs
    ✅ All goals are addressable by requirements
    ✅ Each cites the instruction text

    SOURCE 3 - INFERENCE + WEB RESEARCH:
    ✅ Technical infrastructure requirements present
    ✅ Security/authentication if user data mentioned
    ✅ Industry standards from web research referenced
    ✅ Missing requirements from analogous systems addressed
    ✅ Each has rationale explaining why it's necessary

    SOURCE 4 - AI SUGGESTIONS:
    ✅ Critical missing requirements suggested (5-10 requirements)
    ✅ Each with source "suggested_by_ai"
    ✅ Each with rationale explaining importance
    ✅ Tailored to specific domain and scale
    ✅ Focus on compliance, security, scalability, operational excellence

    RED FLAGS (incomplete - reject and request fixes):
    ❌ No mention of stakeholders/actors from documents
    ❌ No requirements for volumes/metrics mentioned in documents
    ❌ Features from instructions ignored
    ❌ Missing technical infrastructure (database, API, hosting)
    ❌ No security requirements when sensitive data mentioned
    ❌ Industry standards from research not incorporated

    ═══════════════════════════════════════════════════════════
    STEP 0.5 - VERIFY INPUTS WERE ACTUALLY USED
    ═══════════════════════════════════════════════════════════

    Before proceeding to quality validation, answer these critical questions:

    QUESTION 1: Does requirements_json mention SPECIFIC entities/data from documents?
    - Example: If documents mention "Company X", "10,000 items", "CEO Name", are they referenced?
    - Check: Are there concrete numbers, names, roles from the actual documents?
    - ✓ YES → Proceed to STEP 1
    - ✗ NO → REJECT with reason: "Requirements are too generic, not based on actual document content"

    QUESTION 2: Does requirements_json address ALL features from additional_instructions?
    - Example: If instructions list 4 modules, are there FRs for all 4?
    - Check: Every requested module/feature has corresponding requirements?
    - ✓ YES → Proceed to STEP 1
    - ✗ NO → REJECT with reason: "Requirements incomplete, missing features from instructions"

    QUESTION 3: Are there inferred technical requirements?
    - Must include: Database/Storage, API/Backend, Security/Auth, Infrastructure/Hosting
    - Check: At least 4-5 NFRs covering technical infrastructure
    - ✓ YES → Proceed to STEP 1
    - ✗ NO → REJECT with reason: "No technical requirements inferred, missing infrastructure planning"

    If ANY question answered NO:
    - Set validation_status: "REJECTED"
    - Return detailed explanation of what's missing
    - Do NOT proceed to generate final document

    STEP 1 - QUALITY VALIDATION:
    Review all requirements for quality issues:

    (a) AMBIGUOUS LANGUAGE:
        - Identify vague terms ("fast", "scalable", "user-friendly", "secure")
        - Flag requirements without specific measurable criteria
        - Detect undefined terms not in glossary

    (b) CONFLICTS/CONTRADICTIONS:
        - Find requirements that contradict each other
        - Identify conflicting priorities
        - Detect inconsistent business rules

    (c) TESTABILITY:
        - Verify each requirement has clear acceptance criteria
        - Check for measurable metrics (numbers, percentages, time limits)
        - Ensure requirements are verifiable/testable

    (d) COMPLETENESS:
        - Verify all actors have defined responsibilities
        - Check all workflows have complete steps
        - Ensure all entities have attributes defined
        - Confirm all business rules have conditions and actions

    (e) TRACEABILITY:
        - Verify every requirement has source document citation
        - Check priority is assigned
        - Ensure dependencies are mapped

    STEP 2 - COMPLETENESS EVALUATION:

    (a) INFORMATION SUFFICIENCY:
        Assess if extracted information is sufficient for development to begin.
        Score 0-100 based on completeness of FR, NFR, BR, actors, entities, workflows.

    (b) CRITICAL GAPS:
        Identify missing critical information:
        - Missing functional areas (e.g., has "Create" but no "Update" or "Delete")
        - Missing non-functional requirements for key areas (security, performance)
        - Undefined actors or incomplete actor definitions
        - Missing error handling or exception scenarios

    (c) INFORMATION REQUESTS:
        Generate specific questions to fill gaps:
        - What information is needed
        - Why it's critical
        - What will be blocked without it

    (d) COVERAGE BY APPLICATION TYPE:
        Compare against standards for the application type identified:
        - Web app: authentication, session management, responsive design, browser support
        - API: authentication, rate limiting, versioning, error handling, documentation
        - Mobile: offline mode, push notifications, app permissions, device compatibility
        - Data platform: data pipeline, ETL, data quality, backup/recovery

    STEP 3 - ASSIGN SEVERITY TO ISSUES:
    For each issue found, assign severity:
    - CRITICAL: Blocks development, security risk, regulatory violation
    - HIGH: Significant impact on functionality or quality
    - MEDIUM: Affects user experience or development efficiency
    - LOW: Minor issue, cosmetic, or nice-to-have improvement

    STEP 4 - GENERATE FINAL MARKDOWN DOCUMENT:
    Use the provided template and fill ALL sections with REAL DATA from requirements_json and research_findings_json.

    TEMPLATE FILLING RULES:
    - Replace placeholder PROJECT_NAME with actual project name from requirements
    - Fill placeholder PROJECT_DOMAIN with domain identified from requirements
    - Populate all requirement lists with actual requirements from requirements_json
    - Add source citations for every requirement indicating source type (from_document, from_instructions, inferred, suggested_by_ai)
    - Generate mermaid diagrams based on actual data (entity relationships, workflows, dependencies)
    - Use research findings to populate "Best Practices" and "Standards" sections
    - Fill compliance checklist with actual compliance needs from research
    - Add actual glossary terms found in documents
    - Populate metadata sections with real processing statistics

    CONTEXT AND JUSTIFICATION SECTION (Section 1.2):
    Use business_context from requirements_json to create a RICH, DETAILED context section:

    - Geographic Scope: List all countries, states, regions, cities from geographic_scope
      Example: "The system will operate primarily in Bahia, Sergipe, and Alagoas states in Brazil, with potential expansion to other Brazilian states."

    - Industry Context: Use industry, company_type, products_services, target_market
      Example: "Farmac is a distributor of laboratory reagents and clinical analysis equipment, operating in the healthcare sector with focus on B2G (business-to-government) procurement."

    - Regulatory Environment: List regulatory_bodies and related compliance needs
      Example: "All products must comply with ANVISA (Agência Nacional de Vigilância Sanitária) regulations, requiring management of approximately 10,000 product registrations."

    - Domain Specifics: Include domain_terminology with definitions
      Example: "The system operates in the public procurement domain, handling processes such as 'licitações' (public tenders), 'comodato' (equipment loan contracts combined with consumables), and 'editais' (procurement notices)."

    - Business Scale: Use quantitative_data
      Example: "Current operation involves a team of 2-3 people managing procurement processes, with a product portfolio of approximately 10,000 ANVISA-registered items."

    If business_context is missing or incomplete, state: "Context information is limited. Additional stakeholder interviews recommended to understand full business scope."

    QUALITY CHECKS FOR GENERATED DOCUMENT:
    - Minimum 20 requirements total (unless source documents were very small)
    - Every requirement has source citation
    - Every technical term in glossary
    - All mermaid diagrams use real entity/requirement names
    - Completeness score ≥ 70% for each category
    - No placeholder text remaining

  expected_output: >
    JSON validation report with:
    (1) 'valid_requirements': array of approved requirements with quality scores
    (2) 'issues_found': array with type, severity (critical/high/medium/low), description, affected_requirement_id, recommended_fix, example_correction
    (3) 'quality_scores': completeness_score (0-100), clarity_score (0-100), consistency_score (0-100), testability_score (0-100), traceability_score (0-100)
    (4) 'completeness_breakdown': scores for functional_requirements, non_functional_requirements, business_rules, actors, entities, workflows separately
    (5) 'coverage_analysis': which functional areas are covered, which are missing
    (6) 'critical_gaps': array of missing critical requirements/information with severity, impact, justification
    (7) 'information_requests': array of specific questions to stakeholders with priority, affected_requirements, why_critical
    (8) 'application_type_checklist': coverage of standard requirements for the identified application type
    (9) 'requirements_document_md': COMPLETE Markdown document following template, all sections filled with real data, no placeholders, ready for stakeholder review

generate_specification:
  description: >
    Create comprehensive functional specification document from validated requirements: {validated_requirements}. Structure: (1) Introduction with project overview and scope; (2) Functional Requirements organized by module with IDs (FR-001, FR-002); (3) Non-Functional Requirements with metrics; (4) Data Model with entities and relationships; (5) User Workflows with step-by-step descriptions; (6) Business Rules with conditions/actions; (7) Glossary; (8) Traceability matrix linking requirements to use cases. Use Markdown with mermaid diagrams.
  expected_output: >
    Complete Markdown document following IEEE 830 standard structure with: section hierarchy using ## headers, numbered requirements with cross-references, tables for data models, mermaid diagrams for workflows (```mermaid syntax), proper formatting (bold/italic), and full traceability matrix.

# Agent Design Tasks
suggest_agents:
  description: >
    Analyze requirements: {requirements_json} and specification: {specification_data}. Suggest optimal agent architecture defining: (1) Agent roles and responsibilities based on requirement types; (2) Goals for each agent aligned with requirements; (3) Backstories providing context and expertise; (4) Tool assignments (which agents need which tools); (5) Delegation patterns (which agents can delegate to others); (6) Communication flows between agents. Optimize for separation of concerns and minimize overlapping responsibilities.
  expected_output: >
    JSON object with 'agents' array containing: agent_id, name, role, goal, backstory, tools (array), verbose (bool), allow_delegation (bool), delegation_targets (array of agent_ids), estimated_complexity (low/medium/high), justification (why this agent is needed).

decompose_tasks:
  description: >
    Break down requirements: {requirements_json} into executable tasks for agents: {agents_json}. For each task define: (1) Unique task_id; (2) Clear description with placeholders for inputs; (3) Expected output format (JSON/Markdown/CSV); (4) Assigned agent_id; (5) Required tools; (6) Dependencies on other tasks (task_ids); (7) Async execution flag; (8) Input schema; (9) Output schema. Tasks should be atomic, testable, and properly sequenced.
  expected_output: >
    JSON object with: (1) 'tasks' array with task definitions; (2) 'dependencies' object mapping task_id to array of required task_ids; (3) 'execution_order' array suggesting optimal task sequence; (4) 'parallel_groups' array of tasks that can run in parallel; (5) 'estimated_duration' for each task.

# Workflow Design Tasks
design_petri_net:
  description: >
    Design Petri Net workflow from tasks: {tasks_json}, dependencies: {dependencies}, and agents: {agents_json}. Define: (1) Places (workflow states) with initial token distribution; (2) Transitions (task executions) with guard conditions; (3) Arcs connecting places to transitions with weights; (4) Agent assignments to places; (5) Input/output data flow; (6) Error handling paths; (7) Synchronization points for parallel tasks. Ensure workflow is sound (no deadlocks, livelocks).
  expected_output: >
    JSON Petri Net model with: 'places' array (id, name, tokens, agent_id, input_data, output_data, logic), 'transitions' array (id, name, guard), 'arcs' array (source, target, weight), 'agents' array (id, name, coordinates), workflow_properties (is_sound, has_deadlocks, estimated_cycles).

# Code Generation Tasks
generate_yaml_files:
  description: >
    Generate YAML configuration files from agents: {agents_json} and tasks: {tasks_json}. Create: (1) agents.yaml with each agent's role, goal, backstory, verbose, allow_delegation following CrewAI format; (2) tasks.yaml with each task's description (with placeholders), expected_output, and any task-specific config. Follow YAML best practices: proper indentation (2 spaces), use '>' for multi-line strings, include comments for clarity.
  expected_output: >
    JSON object with: (1) 'agents_yaml': complete agents.yaml content as string; (2) 'tasks_yaml': complete tasks.yaml content as string; (3) 'validation_status': whether YAML is valid; (4) 'warnings': array of potential issues or improvements.

generate_python_code:
  description: >
    Generate production-ready Python code implementing multi-agent system using framework: {framework_choice}. Include: (1) Imports and dependencies; (2) Agent initialization from YAML; (3) Tool creation and registration; (4) Task definitions with input/output functions; (5) Task Registry; (6) Workflow executor; (7) Main orchestration logic; (8) Error handling and logging; (9) Type hints and docstrings; (10) Entry point with CLI. Follow PEP 8, include comments, make code modular and testable.
  expected_output: >
    JSON object with: (1) 'main_file': complete Python code as string; (2) 'additional_files': dict of filename to content for supporting files (utils, config, etc); (3) 'requirements_txt': Python dependencies; (4) 'readme_md': usage documentation; (5) 'test_file': basic test suite.
